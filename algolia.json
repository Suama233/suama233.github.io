
[
  
  
  {
    "objectID": "1754568772",
    "permalink": "/post/d2l_5/",
    "title": "[代码笔记]d2l 5",
    
    "content": "\r块与其层的存储\rModule添加子模块时将其存到了一个有序字典里（而不是列表），是为了存键值对后遍历（如初始化参数等时）能快速访问模块\n参数管理\r可以通过如 net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1)) X = torch.rand(size=(2, 4)) net(X) print(net[2].state_dict()) 来获得某一层的所有参数情况，形如：\nOrderedDict([(\u0026#39;weight\u0026#39;, tensor([[-0.2359, 0.1440, 0.1914, 0.2999, -0.2030, -0.2770, 0.0725, 0.2570]])), (\u0026#39;bias\u0026#39;, tensor([-0.0155]))]) 事实上torch中每个参数是一参数类的一个实例的形式存在的\n其包含了值、梯度和一些其它额外信息\n而具体某个参数可以通过进一步操作得到：\nprint(type(net[2].bias)) print(net[2].bias) print(net[2].bias.data) net[2].weight.grad \u0026lt;class \u0026#39;torch.nn.parameter.Parameter\u0026#39;\u0026gt; Parameter containing: tensor([-0.0155], requires_grad=True) tensor([-0.0155]) None #因为还没调用backward 直接设定参数也是可行的\nnet[0].weight.data[:] += 1 net[0].weight.data[0, 0] = 42 net[0].weight.data[0] 可以将一个层对象反复加到一个块里来实现共享参数的效果（参量会同步改变）\nshared = nn.Linear(8, 8) net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1)) 书给出的一些好处： 对于图像识别中的CNN，共享参数使网络能够在图像中的任何地方而不是仅在某个区域中查找给定的功能。 对于RNN，它在序列的各个时间步之间共享参数，因此可以很好地推广到不同序列长度的示例。\n感觉像是某个移动算子（如fileter）的一种模拟移动效果的实现方式？\n参数存储与加载\rtorch.save(net.state_dict(), \u0026#39;mlp.params\u0026#39;) clone = MyLayer() clone.load_state_dict(torch.load(\u0026#39;mlp.params\u0026#39;)) clone.eval() 这是可行且必要的。而参数会以参数字典形式加载\n但建议架构还是用代码实现，其用文件保存、加载可能存在一些问题或麻烦\nGPU初见\r可以通过notebook中执行!nvidia-smi来获得GPU信息 ",
    
    "date": "2025-08-07 20:12:52",
    "updated": "2025-08-07 20:12:52"
  }
  
  , 
  {
    "objectID": "1754395972",
    "permalink": "/post/d2l_4/",
    "title": "[代码笔记]d2l 4",
    
    "content": "\r简述\r涉及到一些基本操作与基本世界观的补充搭建，以及一些数学的再理解\n简单理解.detach()\r它会返回一个新张量，且该张量脱离于原张量附属的计算图而存在，不参与梯度计算\n该张量与原张量共享存储空间，意味着in-place modifications对二者是同步的\n可以用来如数据呈现、记录等\n例：\nx = torch.arange(-8.0,8.0,0.1,requires_grad=True) y = torch.relu(x) d2l.plot(x.detach(),y.detach(),\u0026#39;x\u0026#39;,\u0026#39;relu(x)\u0026#39;,figsize=(5,2.5)) 区分torch.relu(x)与torch.nn.functional.relu(x)\r其实没什么区别，更像是通过更改名字来区分与统一应用场景，二者最终指向的c代码也是同一部分 其实还有torch.nn.ReLU等，其更多是作为层对象存在的 可以理解为：torch不但可以构建网络，其本身也提供了大量的基础工具，而恰好两个功能中有工具一样 Xavier初始化\r会将来单独写一篇\n",
    
    "date": "2025-08-05 20:12:52",
    "updated": "2025-08-05 20:12:52"
  }
  
  , 
  {
    "objectID": "1754309572",
    "permalink": "/post/d2l_3_57/",
    "title": "[代码笔记]d2l 3.5-3.7",
    
    "content": "\r源代码\rimport torch import matplotlib.pyplot as plt import random from utils import * %matplotlib inline def synthtic_data(w,b,num_examples): X = torch.normal(0,0.5,(num_examples,len(w))) y = torch.matmul(X,w) + b return X,y.reshape((-1,1)) true_w = torch.tensor([-3.,-4.2]) true_b = 3.4 features,labels = synthtic_data(true_w,true_b,1000) print(f\u0026amp;#34;features: {features[0]},\\nlabels: {labels[0]}\u0026amp;#34;) plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1) def data_iter(batch_size,features,labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) for i in range(0,num_examples,batch_size): batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)]) yield features[batch_indices], labels[batch_indices] batch_size = 10 for X,y in data_iter(batch_size,features,labels): print(X,\u0026amp;#34;\\n\u0026amp;#34;,y) break w = torch.normal(0,0.01,size = (2,1),requires_grad=True) b = torch.zeros(1,requires_grad=True) def linreg(X,w,b): return …",
    
    "date": "2025-08-04 20:12:52",
    "updated": "2025-08-04 20:12:52"
  }
  
  , 
  {
    "objectID": "1753963972",
    "permalink": "/post/d2l_3_23/",
    "title": "[代码笔记]d2l 3.2-3.3",
    
    "content": "\r源代码\rimport torch import matplotlib.pyplot as plt import random from utils import * %matplotlib inline def synthtic_data(w,b,num_examples): X = torch.normal(0,0.5,(num_examples,len(w))) y = torch.matmul(X,w) + b return X,y.reshape((-1,1)) true_w = torch.tensor([-3.,-4.2]) true_b = 3.4 features,labels = synthtic_data(true_w,true_b,1000) print(f\u0026amp;#34;features: {features[0]},\\nlabels: {labels[0]}\u0026amp;#34;) plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1) def data_iter(batch_size,features,labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) for i in range(0,num_examples,batch_size): batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)]) yield features[batch_indices], labels[batch_indices] batch_size = 10 for X,y in data_iter(batch_size,features,labels): print(X,\u0026amp;#34;\\n\u0026amp;#34;,y) break w = torch.normal(0,0.01,size = (2,1),requires_grad=True) b = torch.zeros(1,requires_grad=True) def linreg(X,w,b): return …",
    
    "date": "2025-07-31 20:12:52",
    "updated": "2025-07-31 20:12:52"
  }
  
  , 
  {
    "objectID": "1753843252",
    "permalink": "/post/veritigo/",
    "title": "[乱七八糟]《迷魂记》简单感想",
    
    "content": "对斯考蒂的玛德琳理型的毁灭以及用真正的坠亡将“破灭”这一意义变得不可被任何解释更改或被记忆以非遗忘的修方式修复。\n斯考蒂在“朱迪”上复原玛德琳的举动，我理解为在现实中强行塑造玛德琳这一理型。而“玛德琳”，由于附身症状与坠楼的与经验事实背离，辅以时间来抹杀除斯考蒂脑外对她的存在证明，彻底被他从现实萃取到了理型。那么这种强硬的延续本身其实也是蕴含着一种自毁冲动：即使没有挂坠，朱迪与斯考蒂也不会像玛德琳与斯考蒂，这是显而易见的。而斯考蒂的偏执在那时必然毁灭自己或非自己（极大可能是朱迪，她的对抛弃自己身份变成被演绎的假人本身比起爱情更像是半自愿的自毁）。\n我可以认为斯考蒂制造玛德琳的行为本身是在尝试向自己论证玛德琳的理型的不可制造性吗？至少我觉得他的潜意识一部分是像证明这一点的。通过证伪对心中事实的一次反驳来进一步加强对其的坚信，但这个行为被项链完全破坏了，它完全证明了被人造的质疑是真的，而且整个逻辑链是自己做出来的。这种破碎基本等价于一种对斯考蒂人格的污蔑。\n而面对这个污蔑，斯考蒂二元地有两种路：否定为事实的污蔑，这相当于完全地投入癔症，用变成精神病做影片结尾似乎怎么说都有点不太合适；另一种是接受污蔑，承认自己做错了一步并在此基础上重新开始。\n我更想将其理解为：将一个事件的结局失败更改为一个更大事件的中途的挫折？这样给了受挫者重新开始并在臆想的成功节点时将这个失败改写为经验，或“成功的基石”。那么接受朱迪，告诉自己她有成为玛德琳，甚至是她自己与玛德琳的更好的结合的可能，或者什么我没理解出来的渴望（能感觉到这是一种渴望，但这种渴望被投射到的地方感觉还有待讨论），进而将其转化为间奏，这似乎是更现实的、更有积极一面的出路。\n然后朱迪坠亡了，两个出路一个被斯考蒂自己否定一个被并不意外的意外扼杀，这个结局说实话太欧亨利了，感觉真够学很久的。\n也就是说他的这个献上自己作为人的存在的故事被在此处真真正正地结束掉了。可以说斯考蒂的人格也坠亡了。所谓的克服了恐高，只不过是因为向下看的那个人长得像斯考蒂、名字也恰好叫斯考蒂而已。他当然不会一起掉下去，但他确实已经死了。\n以《西哈诺·德·贝热拉克》的一句话做结尾：\n“我们信里的梦中情人往往是随口想个名字，用幻想唱出来的泡沫！”\n但把信寄出去的话，真的能使美梦成真吗？\n",
    
    "date": "2025-07-30 10:40:52",
    "updated": "2025-07-30 10:40:52"
  }
  
  , 
  {
    "objectID": "1745757712",
    "permalink": "/post/leetcode_intro_to_pandas/",
    "title": "[学习随笔] Pandas入门",
    
    "content": "本文使用语言：Python（有点多余了\u0026amp;hellip;）\n创建DataFrame\r从数组到DataFrame\r二维数组可以通过pd.DataFrame(list_name)直接转为DataFrame,不输入参数时默认列名为range(0-n)\n可以通过指定参数columns = \u0026amp;lt;list\u0026amp;gt;,index = \u0026amp;lt;list\u0026amp;gt;来修改列、行名，，其中\u0026amp;lt;list\u0026amp;gt;为python列表，如：\ndef createDataframe(student_data: List[List[int]]) -\u0026amp;gt; pd.DataFrame: ans_df = pd.DataFrame(student_data,columns = [\u0026amp;#39;id\u0026amp;#39;,\u0026amp;#39;age\u0026amp;#39;]) return ans_df 输入格式如：[[1,15],[2,11],[3,11],[4,20]]\n输出格式如：{\u0026amp;quot;headers\u0026amp;quot;: [\u0026amp;quot;id\u0026amp;quot;, \u0026amp;quot;age\u0026amp;quot;], \u0026amp;quot;values\u0026amp;quot;: [[1, 15], [2, 11], [3, 11], [4, 20]]}\n标准输出格式：（如print(df)）\nid age 0 1 15 1 2 11 2 3 11 3 4 20 对DataFrame的基本操作\r查询形状\r已知list_df是一个DataFrame，可以通过调用它的shape属性来获取其尺寸，返回对象类型是tuple\n与之对比的是size属性。它返回的是具体内容的个数\nplayers = [[1,20],[2,21],[3,22]] players_df = pd.DataFrame(data = players, columns = [\u0026amp;#39;Id\u0026amp;#39;, \u0026amp;#39;Age\u0026amp;#39;]) print(player_df.shape) #-----\u0026amp;gt;(3,2) ## 有三行二列 print(player_df.size) #-----\u0026amp;gt;6 ## 有六个元素（行列名未被算进去，统计占了多少“格子” 显示前x行\rdata_df.head(x) 可以显示该DataFrame的前x行\n如果不传参，默认显示前5行\n定位特定位置\r有两种主要定位方式：基于标签与基于整数\n基于标 …",
    
    "date": "2025-04-27 20:41:52",
    "updated": "2025-04-27 02:26:52"
  }
  
  , 
  {
    "objectID": "1745622472",
    "permalink": "/post/binary_search/",
    "title": "[琐碎简记]二分查找、离散边界确认与终止点选择",
    
    "content": "注：本篇使用语言：C++\n基本背景与类似情况\r二分查找原理本身很简单：对一顺序数组取中间值，比较其与目标大小，让中间值成为新边界，二分缩短区间。此过程迭代进行，直到找到或找不到目标，时间复杂度$O(logn)$\n典型的类似情况是快排，因为二者本质上都是在选取中间值并更新边界的过程。\n具体分析与处理\r对中间索引的微处理\r理想状态而言应该是一直选取到中间，然后更新：比如在目标值大于中间值时，更新左边界为中间本身，右边界不变。如此往复直到夹逼出一个数\n但索引本身是离散的，这导致的问题是数学算式mid=(left+right)/2可能出现索引mid是浮点数的情况，毕竟卡到两数中间时无法取值。而代码中mid的唯一写法也只能是mid=(left+right)/2（一个等效写法是(right-left)/2+left，二者完全相同，不展开），这会导致mid其实既可能是正中间，也可能是中间稍左的位置。\n注：两种算中点的方式还真不同：前者先算left+right是有溢出风险的\u0026amp;hellip;还是后者好些\n如果递归函数传参时直接将mid本身传进去，就会导致在只剩两个数时无法继续更新的情况（mid会一直与左边界相等）。而递归函数一般来说做特判都是逻辑对全过程不统一然后出bug的前兆，故应该意识到理论和实际是有差距的，需要具体情况进行微调。\n解决方法是利用离散本身的特点：“mid需要成为下一次递归的边界”本身已经暗示了一个事实：其对应的数已经被判断过了。在连续情况下这代表着mid是一个开区间而非闭区间，但因为其对应的位置本身不变而有被忽视的可能，但在这里，开闭直接决定了了传入的是mid±1还是其本身。\n也就是说，原来的方法的错误原因是，每次递归都会进行一次多余的比较，而该次比较是能对是否继续递归产生影响的。也就是说，还是逻辑没有捋清楚导致的。\n对终止判断的选取\r如果能考虑到mid更新时需要手动±1，这里会好想到些，不过或许还是积累记住更为重要？\n递归函数的终止是难点。除去比较好理解的因为达到目的而结束递归，另一种对无法达成目的的判断更不好想到（比如这里可能目标根本不在数组里）\n考虑到二分缩减区间的直观解释，无法达成显然代表着即使区间缩完也没有找到。而区间缩完也就代表着左边界等于右边界。\n这是必然能达成的吗？由于mid的最小移动量（相对于左边界）是0（靠奇数除2后舍去小数），而每次mid只会 …",
    
    "date": "2025-04-26 07:07:52",
    "updated": "2025-04-26 07:12:52"
  }
  
  , 
  {
    "objectID": "1744893712",
    "permalink": "/post/smart_ptr/",
    "title": "[学习随笔] C++智能指针",
    
    "content": "\r三种智能指针\r概述：是什么，及其参与的生态位\rC++ 11后，标准库（std）中新加入了四种指针：unique_ptr,shared_ptr,weak_ptr,auto_ptr(最后一种在C++17 后被废弃因而本文不做任何涉及)。它们对应的传统位置是C++的裸指针（和前者相比，纯粹的一个T* ptr没有被封装，故得名）。它们的创建与删除操作对应了裸指针的new,delete操作。也就是说，它的核心任务是在堆上人为分配资源，但同时利用了栈与作用域的特性，实现了更安全的资源分配。\n安全在哪？\r裸指针指针每次创建都需要手动调用delete删除。这可能会有以下问题： 忘记delete 当程序逻辑很复杂时，找不对调用delete的时机 delete错东西（指针指向发生变化后原来指向的内存找不回来却也不能再被分配，或如链表等在删除时少删了几个结点） 既然程序结束时一定其创建的资源会被卸载，不能不管它吗？ 显然不能。程序存在逻辑漏洞 对于长期运行程序，没有靠这个卸载的机会 这种结束程序强行截断可能导致有些模块逻辑没闭环，留下可以被外界接入的契机 程序发生意外时，可能有些内存被强制卸载为下次启动留下bug（类似上一条）\n而相对的，智能指针能只管创建，而可以在其所属作用域结束后自动卸载。这样就不用考虑忘记或删错东西的情况。同时它还能在遭遇意外时也自动执行delete，这也就是“异常安全”保证（异常不是形容词）\n如果有些对象生存周期和程序本身一样（或中途创建，到程序结束时才被卸载），那它和等到主函数结束让系统自动删有区别吗？\n显然有的。让程序出发与结束时同状态是正常设计该有的事，而不能将这种是交由系统的强行执行（就像重写构造、析构函数） 对于这些资源，是“它们的生存周期本来就该到程序结束”，这和“忘了删”形式上一样，本质上不同 需要考虑到将来这些资源需要提前被卸载的可能。用智能指针是能让自己有掌控它被卸载的时机的（这个可能体现在运行逻辑设计上而不是非要自己明显地意识到） 该何时用智能指针？\r从目前学到的来看，应该是“尽量用”。\n也就是：在要用指针是首先考虑用unique_ptr，如果其在功能上不够或在含义上有更契合的，则考虑后两种智能指针，总之不是裸指针\n智能指针（尤其unique_ptr）的开销是很小的，故不用在这方面节省性能\n简单对比三种智能指针\r首先在OOP情境下许多模块类需 …",
    
    "date": "2025-04-17 20:41:52",
    "updated": "2025-04-18 02:26:52"
  }
  
  , 
  {
    "objectID": "1678903200",
    "permalink": "/post/hello/",
    "title": "Hello World",
    
    "content": "ここにいるよ、わたしたち。\n",
    
    "date": "2023-03-15 11:00:00",
    "updated": "2023-03-15 11:00:00"
  }
  
]